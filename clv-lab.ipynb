{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Lifetime Value Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Package Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Doing the necessary installations. You can install these from your notebook or \n",
    "# from a terminal window if you are prompted for a password. \n",
    "\n",
    "# You may have to do `sudo pip ...` for all of the packages below. \n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y r-base\n",
    "!pip install numpy==1.12.0 \n",
    "!pip install pandas==0.19.2\n",
    "!pip install scipy==0.18.1\n",
    "!pip install matplotlib==2.0.0\n",
    "!pip install dill==0.2.6\n",
    "!pip install boto3\n",
    "\n",
    "# PyMC3 allows us to build probabilistic models\n",
    "!pip install pymc3==3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import gc\n",
    "import os \n",
    "import sys \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import dill\n",
    "from datetime import datetime \n",
    "from hashlib import md5\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from re import sub\n",
    "import tarfile\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "\n",
    "def s3_fetch_module(s3_path, file_name):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Fetch a module in a tar.gz file from s3\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    bucket = 'ds-cloud-cso'\n",
    "\n",
    "    s3 = boto3.client(service_name = 's3', config=Config(signature_version=UNSIGNED)) \n",
    "                      \n",
    "\n",
    "    \n",
    "    print('Fetching ' + s3_path + file_name)\n",
    "    s3.download_file(Bucket=bucket, Key=s3_path + file_name, Filename=file_name)\n",
    "\n",
    "                \n",
    "    dir_name = sub('.tar.gz$', '', file_name)\n",
    "    \n",
    "    contains_hyphen = False\n",
    "    if '-' in dir_name:\n",
    "        contains_hyphen = True\n",
    "        print(\"Module name contains invalid '-' hyphens.  Replacing with '_' underscores\")\n",
    "        dir_name = dir_name.replace('-','_')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree('./' + dir_name)\n",
    "        print('Removing old module ' + dir_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print('Extracting ' + file_name + ' into ' + dir_name)\n",
    "    archive = tarfile.open(file_name, 'r:gz')\n",
    "    archive.extractall('./')\n",
    "    archive.close()\n",
    "    \n",
    "    if contains_hyphen:\n",
    "        os.rename(dir_name.replace('_','-'), dir_name)\n",
    "        \n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "        print('Removing ' + file_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if ~os.path.exists(dir_name + '/__init__.py'):\n",
    "        print('__init__.py not found.  Creating it in ' + dir_name)\n",
    "        open(dir_name + '/__init__.py','w').close()\n",
    "        \n",
    "    \n",
    "storage_dir = 'expertise-seminars/CLV/Modules/'\n",
    "s3_fetch_module(s3_path = storage_dir, file_name = 's3_helpers.tar.gz')\n",
    "s3_fetch_module(s3_path = storage_dir, file_name = 'plotting_helpers.tar.gz')\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions to fetch files and do some plotting\n",
    "from s3_helpers import functions as shf\n",
    "from plotting_helpers import functions as phf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "For those not familiar with MCMC, it's simply a method to fit a distribution via simulation.  We highly encourage you to read more on this method as it can be broadly useful across many of your modeling efforts.  Here we'll give an intuitive description.\n",
    "\n",
    "**Our goal is to infer or fit a set of parameters, $\\theta$ of our model.  We first define a prior distribution for $\\theta$ (e.g. our belief about $\\theta$ before seeing our data), and then update that prior with our observed data using Bayes Theorem (see Eqn. below) to compute a posterior distribution for $\\theta$.  The posterior is our belief about $\\theta$ after observing the data we wish to fit, and can be thought of as the $\\theta$ 'fitted' to (or inferred from) our data.**\n",
    "\n",
    "$$P(\\theta|Data) = \\frac{P(Data|\\theta)P(\\theta)}{P(Data)}$$\n",
    "**The problem is that often times it's impossible or difficult to compute this posterior distribution analytically.**  How else can we go about determining the posterior distribution?  **One way is to draw samples from the posterior distribution for $\\theta$.**  Imagine that we have a coin, and we don't know whether or not it's biased - in other words we don't know the distribution of heads and tails.  However, we can still flip that coin (draw random samples from that distribution - the *Monte Carlo* part of MCMC), and use those samples to determine the distribution of heads and tails.  We can use this same process to determine the posterior distribution for our model parameters.  **What we need is a way or process that will allow us to repeatedly randomly sample from the posterior even though we don't yet know what that distribution **, akin to flipping the coin in our example.\n",
    "\n",
    "**A *Markov Chain* can be used to allow you to do just that.  Broadly speaking, in the context of an MCMC simulation, a Markov chain is a process that tells with what probability to draw samples of each possible value for $\\theta$ given the last sample drawn.  For example, in the image below, if we sampled 1, then we have a 0.3 probability to sample 2 next, a 0.3 probability to sample 5 next, and a 0.4 probability to sample 4 next.  Let's say we happened to sample 2 next.  Then we now have a probability of 1 to sample 3 next. And so on and so on.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://s3-us-west-2.amazonaws.com/ds-cloud-cso/expertise-seminars/CLV/images/markov_chain.png'; style = 'height:400px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov Chains can also have equilibria, meaning that after drawing a large number of samples, all subsequent samples drawn will come from specific distribution.  It's possible, and surprisingly quite easy to construct a Markov Chain with an equilibrium distribution that matches the posterior distribution of a parameter we want to fit. We can them simply draw samples using this Markov Chain (after throwing out samples that we drew prior to reaching this equilibrium distribution), to build out the posterior distribution of interest.** Essentially you can think of sampling with the Markov Chain as flipping the coin in our example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://s3-us-west-2.amazonaws.com/ds-cloud-cso/expertise-seminars/CLV/images/mcmc.gif'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we won't go into how to construct such a Markov Chain, the simplest and most intuitive algorithm to accomplish this is called the Metropolis-Hastings algorithm, and we encourage you to give it a look.  In this lab, we'll be using the No U-Turn Sampler (NUTS) algorithm, which accomplishes the same thing but is much more efficient for most situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# An Intro to Predictive Modeling for Customer Lifetime Value (CLV) -- Tutorial Notebook \n",
    "\n",
    "In this notebook, you will be introduced to the workflow necessary to train a Pareto/NBD model (e.g. Schmittlein et al. 1987) on a transactional dataset. An extension to the Pareto/NBD model includes predictions for the monetary value as well (Gamma-Gamma model -- Fader et al. 2004). \n",
    "\n",
    "The Pareto/NBD model is a good introductory probabilistic model to the non-contractual setting with continous purchase opportunity. It's a simple enough model that is easy to train and generally produces good results when the assumptions behind the model are met. It's a good first shot at CLV modeling ! \n",
    "\n",
    "\n",
    "## A few words on the CDNOW Dataset \n",
    "The CDNOW dataset is a very popular dataset used in academic papers addressing CLV models. CDNOW used to be an online retailer of CDs in the 1990's. The dataset in question includes the transactional data of a cohort of customers who have made their first purchase in the first quarter of 1997. All transactions from these customers between their purchase and June 1998 are included. The transactional data was downsampled to contain transactions of 10% of the customers population (2357 customers). \n",
    "\n",
    "The CDNOW dataset is a good example of a non-contractual setting with a continuous purchasing opportunity. It has been used extensively in the CLV literature.\n",
    "\n",
    "\n",
    "## Requirements \n",
    "\n",
    "We recommend 8 Gb of Ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Training Pareto/NBD and Gamma/Gamma Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the models, we can use either Maximum Likelihood Estimation (MLE) or Markov Chain Monte Carlo (MCMC) methods.  Each method has strengths and weaknesses.\n",
    "\n",
    "\n",
    "MCMC is more flexible and allows better tuning of the model.  It also allows us to use prior distributions in order to bring in domain-specific knowledge we may have or knowledge from different data sets.\n",
    "\n",
    "MCMC is a numerical simulation method and doesn't require derivation of the analytic form of the model.  Therefore, it's easier to set up so long as we can specify the distributions we wish to fit.  \n",
    "\n",
    "MCMC can handle models that may not have analytic forms at all.\n",
    "\n",
    "MLE is much faster.  MCMC can take hours or even days depending on the model and data.  MLE takes a few seconds.\n",
    "\n",
    "MLE has fewer knobs for tuning the model, and while this makes it inflexible it also means there's less things to worry about when setting it up.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Import the dataset into a Pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data_dir = 'expertise-seminars/CLV/'\n",
    "\n",
    "#Read users file:\n",
    "kwargs = { #pandas read_csv arguments\n",
    "    'names': ['cust','date','sales'], #column names\n",
    "    'sep' : ',', #delimiter\n",
    "    'encoding' : 'latin-1', #file encoding\n",
    "    'header': 0\n",
    "}\n",
    "transactions = shf.s3_CSVtoDF(data_dir + 'cdnow_transaction_log.csv', use_creds = False, \n",
    "                              **kwargs)\n",
    "\n",
    "# Lets convert the data field into a datetime object : \n",
    "transactions['date'] = pd.to_datetime(transactions['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "The dataframe has the familiar structure of a transactional dataset. There is a column for customer ID, a transaction date and the amount of the transaction. We have everything we need to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "Some sales values in the data are $0.  For our model we don't want to consider these as sales, so we'll remove them.  Note that for the Gamma-Gamma monetary value model we'll fit, some implementations of the gamma distribution don't handle values of 0 well.  This includes gamma distributions in packages like SciPy and PyMC3, in which case adding a small increment to all monetary values can get around this issue.  Modules like PySTAN have gamma distribution implementations that are able to work with 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions = transactions[transactions['sales']!=0]\n",
    "transactions['sales'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot raster of customer purchases\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "plt.rcParams['font.size'] = 15\n",
    "phf.raster(transactions, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Exploration of the Transactional Data \n",
    "\n",
    "Here we're going to do some basic exploration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total number of transactions \n",
    "print('Number of Transactions:  ' + str(len(transactions)))\n",
    "\n",
    "# Number of customers : \n",
    "\n",
    "print('Number of Customers: ' + str(len(np.unique(transactions['cust']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# date range of the dataset : \n",
    "print('Min Date: ' + str(transactions['date'].min()))\n",
    "print('Max Date: ' + str(transactions['date'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "The dataset spans 1.5 year of data. A natural breakdown would be to train on one year of data and validate on the following 6 months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select training and holdout periods \n",
    "# Lets select a training period of one year and and a holdout period of 6 months. \n",
    "\n",
    "end_training_time = pd.to_datetime('1997-12-31') # this date is 1 year after first purchase\n",
    "train = transactions[transactions.date <= end_training_time]\n",
    "holdout = transactions[transactions.date > end_training_time]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Compute the RFM Dataframe \n",
    "\n",
    "In this section, we will transform our transactional data into the recency-frequency-monetary value (RFM) dataframe. \n",
    "\n",
    "* Recency : time between first and last transaction\n",
    "* Frequency : here frequency really refers to repeat frequency, i.e. the number of purchases beyond the initial one. i.e. repeat frequency = purchase counts - 1 )\n",
    "* monetary value : mean of all the transactions in the training periods \n",
    "* T : time between first purchase and end of calibration period\n",
    "\n",
    "This is the form in which our model will consume the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rfm(transactions, end_training_time):\n",
    "    \n",
    "    def get_single_cust_rfm(transactions):\n",
    "\n",
    "        # Compute monetary value, the average value\n",
    "        # of all of a customer's transactions\n",
    "        monetary_value = transactions['sales'].mean()\n",
    "\n",
    "        # Compute the difference between the last day \n",
    "        # in the data and a customer's first purchase day\n",
    "        T = (end_training_time - transactions['date'].min()).days\n",
    "\n",
    "        # Compute Recency, the difference between a customer's\n",
    "        # last and first purchase day\n",
    "        recency = ######\n",
    "\n",
    "        # Compute repeat frequency, the total number of\n",
    "        # a customer's purchases - 1\n",
    "        # We can count the number of transactions for a customer\n",
    "        # by counting the number of rows in 'transactions'\n",
    "        frequency = ######\n",
    "\n",
    "        return {'recency': recency, 'frequency': frequency, \n",
    "                'monetary_value': monetary_value, 'T': T}\n",
    "\n",
    "\n",
    "    # get the unique customer IDs\n",
    "    unique_customers = np.unique(transactions['cust'])\n",
    "    \n",
    "    # Initialize RFM dataframe\n",
    "    rfm = pd.DataFrame(index=range(0, len(unique_customers)), \n",
    "                       columns = ['cust', 'recency', 'frequency', 'T', 'monetary_value'],\n",
    "                       dtype = np.float64)\n",
    "\n",
    "    # Compute the RFM for each customer and add it to the RFM dataframe\n",
    "    for ci, cust in enumerate(unique_customers):\n",
    "\n",
    "        c_idx = np.where(transactions['cust']==cust)[0]\n",
    "        rfm.iloc[ci] = get_single_cust_rfm(transactions.iloc[c_idx])\n",
    "\n",
    "    # Add the customer IDs to the RFM dataframe\n",
    "    rfm['cust'] = unique_customers\n",
    "    \n",
    "    return rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's compute the RFM dataframe\n",
    "rfm = compute_rfm(train, end_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the range of values for each column. Make sure the results make sense \n",
    "# before going any further with the analysis. No NaNs, \n",
    "# no negative values, no recency > 364 days, etc.  \n",
    "\n",
    "rfm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "The results look good so far! You could also take a look at the distribution of recency, frequency, T, and monetary value. before going any further. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Train a simple Pareto/NBD Model over the training/holdout period. \n",
    "\n",
    "Now that we have an RFM vector for each customer and for the training period (`rfm` dataframe above), let's train the Pareto/NBD model. For this step, we're going to use PyMC3. <a href=\"https://pymc-devs.github.io/pymc3/index.html\">PyMC3</a> is a package for building probabilistic models.  Alternatives include STAN (though PySTAN module), and emcee. \n",
    "\n",
    "The individual-level likelihood function of the Pareto/NBD model can be easily derived (e.g. Schmittlein et al. 1987; Fader et al. 2005) and will be used in the code below :\n",
    "$$L(\\lambda, \\mu | x, t_x, T) = \\frac{\\lambda^x \\mu}{\\lambda+\\mu}e^{-(\\lambda+\\mu)t_x}+\\frac{\\lambda^{x+1}}{\\lambda+\\mu}e^{-(\\lambda+\\mu)T} \\tag{1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PyMC3 doesn't come with a Pareto/NDB likelihood\n",
    "# We can define a custom likelihood function as shown here\n",
    "# This allows us to build custom distributions and gives us\n",
    "# freedom to specify any model we wish\n",
    "class PNBD(pm.Continuous):\n",
    "    \n",
    "    #Pareto/NDB has parameters mu and lambda\n",
    "    def __init__(self, mu, la, *args, **kwargs):\n",
    "        super(PNBD, self).__init__(*args, **kwargs)\n",
    "        self.mu = mu\n",
    "        self.la = la\n",
    "\n",
    "    # Special function name logp is recognized by pymc3\n",
    "    # as the log likelihood of the distribution\n",
    "    # See Equation 1 above\n",
    "    def logp(self, obs_freq, obs_rec, obs_T):\n",
    "        \n",
    "        # The two terms of the log of likelihood we described previously\n",
    "        like1 = np.multiply(obs_freq, np.log(self.la)) + np.log(self.mu) - np.log(self.mu+self.la) - np.multiply(obs_rec, (self.mu+self.la))\n",
    "        like2 = np.multiply((obs_freq + 1), np.log(self.la)) - np.log(self.mu+self.la) - np.multiply(obs_T, (self.la+self.mu))\n",
    "\n",
    "        return np.log(np.exp(like1) + np.exp(like2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously discussed, $\\lambda$ is the count rate that goes in the Poisson distribution and $\\mu$ is the slope of the lifetime exponential distribution. The typical lifetime corresponds to $\\sim 1/\\mu$. \n",
    "\n",
    "The priors for $\\lambda$ and $\\mu$ are gamma distributed : \n",
    "$$g(\\lambda|r,\\alpha) = \\frac{\\alpha^r}{\\Gamma(r)}\\lambda^{r-1}e^{-\\lambda \\alpha} \\tag{2}$$\n",
    "and \n",
    "$$g(\\mu|s,\\beta) = \\frac{\\beta^s}{\\Gamma(s)}\\mu^{s-1}e^{-\\mu \\beta} \\tag{3}$$ \n",
    "\n",
    "For each of the four model parameters $(r,\\alpha,s,\\beta)$, we assign hyperpriors that are normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cust = len(rfm) #Store number of customers\n",
    "with pm.Model() as pareto_nbd:\n",
    "\n",
    "\n",
    "    # We want r, alpha, s, and beta to have normal priors\n",
    "    # but values less than 0 aren't valid here, so we define a\n",
    "    # normal distribution bounded at 0\n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=np.array(0))\n",
    "    \n",
    "    # Mu and Lambda values > 1 don't make sense in our context,\n",
    "    # so we'll define a gamma distribution that is bounded at 1\n",
    "    BoundedGamma = pm.Bound(pm.Gamma, lower=np.array(0), upper=np.array(1))\n",
    "\n",
    "\n",
    "    #--Global (population) parameters--#\n",
    "    # Hyperpriors for parameters for the\n",
    "    # gamma prior distribution of lambda\n",
    "    # See Equation 2\n",
    "    r = BoundedNormal('r', mu = 1, sd = 10);\n",
    "    alpha = BoundedNormal('alpha', mu = 10, sd = 1000);\n",
    "    \n",
    "    # Implement hyperpriors for the parameters\n",
    "    # for the gamma distribution for mu\n",
    "    s = ######\n",
    "    beta = ######\n",
    "   \n",
    "    #--Customer parameters--#\n",
    "    # Gamma prior distribution of lambda\n",
    "    # as shown in Equation 2\n",
    "    # Note the shape argument - this tells PyMC3 that\n",
    "    # we want one lambda for each customer\n",
    "    # Without this argument, a single lambda would be computed\n",
    "    # for everyone\n",
    "    la = BoundedGamma('la', \n",
    "                      alpha = r,\n",
    "                      beta = alpha,\n",
    "                      shape = n_cust\n",
    "                      );\n",
    "    \n",
    "    # Implement the gamma prior distribution for \n",
    "    # mu here, with parameters s and beta\n",
    "    # as shown in Equation 3\n",
    "    mu = ######\n",
    "    \n",
    "    # Pareto / NBD likelihood.  Mu and Lambdas(La) are fitted\n",
    "    # frequency, recency, and T are observed from the data\n",
    "    # mu and la parmeters are estimated for each customer\n",
    "    ylike = PNBD('ylike', mu = mu, la = la, observed = {'obs_freq': rfm['frequency'].values, \n",
    "                                                      'obs_rec': rfm['recency'].values, \n",
    "                                                      'obs_T': rfm['T'].values}, shape = n_cust) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "burn_in = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with pareto_nbd:\n",
    "#    trace = pm.sample(draws = 10000, init = None, tune = burn_in, chain = 1, step = pm.NUTS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('paretonbd.pkl', 'wb') as f:\n",
    "#    dill.dump(trace, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#local_file = 'paretonbd.pkl'\n",
    "#target_file = 'expertise-seminars/LTV/paretonbd.pkl'\n",
    "#shf.push_file_to_s3(path = local_file, key = target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_file = '/home/jupyter/paretonbd.pkl'\n",
    "target_file = 'expertise-seminars/CLV/paretonbd.pkl'\n",
    "shf.pull_file_from_s3(target_file, local_file, use_creds = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/jupyter/paretonbd.pkl', 'rb') as f:\n",
    "    trace = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MCMC methods return distributions of parameters rather than single values\n",
    "# To get a single estimate we can take the mean, median, or mode\n",
    "# or some other representative measure from the distribution of that parameter\n",
    "\n",
    "# Here, we'll use the median to account for some skewness in the distributions\n",
    "\n",
    "r = np.median(trace[burn_in:].get_values(varname='r'))\n",
    "alpha = np.median(trace[burn_in:].get_values(varname='alpha'))\n",
    "s = np.median(trace[burn_in:].get_values(varname='s'))\n",
    "beta = np.median(trace[burn_in:].get_values(varname='beta'))\n",
    "\n",
    "\n",
    "# Population level parameter distributions\n",
    "# and their medians, which we take as the \n",
    "# estimated parameter value\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "\n",
    "ax1.hist(trace[burn_in:].get_values(varname='r'), normed = True, bins = 200);\n",
    "ax1.set_title('r')\n",
    "ax1.vlines(r, 0, 12)\n",
    "\n",
    "ax2.hist(trace[burn_in:].get_values(varname='alpha'), normed = True, bins = 200);\n",
    "ax2.set_title('alpha')\n",
    "ax2.vlines(alpha, 0, 0.1)\n",
    "\n",
    "ax3.hist(trace[burn_in:].get_values(varname='s'), normed = True, bins = 200);\n",
    "ax3.set_title('s')\n",
    "ax3.vlines(s, 0, 6)\n",
    "\n",
    "ax4.hist(trace[burn_in:].get_values(varname='beta'), normed = True, bins = 200);\n",
    "ax4.set_title('beta')\n",
    "ax4.vlines(beta, 0, 0.02)\n",
    "\n",
    "print('Population Parameters:')\n",
    "print('----------------------')\n",
    "print('r:\\t\\t' + str(r))\n",
    "print('alpha:\\t\\t' + str(alpha))\n",
    "print('s:\\t\\t' + str(s))\n",
    "print('beta:\\t\\t' + str(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Customer-level parameters\n",
    "# Each customer has a unique distribution for mu and lambda\n",
    "# We'll take the mean of each customer's distribution\n",
    "# as the estimate of that customer's mu and lambda parameters\n",
    "\n",
    "# Below we show the distribution of the mu and lambda estimates for all customers\n",
    "print('Customer-level Parameters (First 5 Customers):')\n",
    "print('----------------------------------------------')\n",
    "# Note that we still need to take means\n",
    "# to get each parameter value, since each\n",
    "# user has a distribution for each user-specific parameter.\n",
    "\n",
    "mu_dist = trace[burn_in:].get_values(varname='mu')\n",
    "la_dist = trace[burn_in:].get_values(varname='la')\n",
    "\n",
    "mean_mus = mu_dist.mean(axis=0)\n",
    "mean_las = la_dist.mean(axis=0)\n",
    "mu_la_df = pd.DataFrame({'cust': rfm['cust'], \n",
    "                         'mu:' : mean_mus,\n",
    "                        'la': mean_las})\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(trace[burn_in:].get_values(varname='mu').mean(axis = 0), bins = 100);\n",
    "ax1.set_title('mu')\n",
    "ax1.set_ylabel('Number of Customers')\n",
    "ax1.set_xlabel('mu parameter')\n",
    "\n",
    "ax2.hist(trace[burn_in:].get_values(varname='la').mean(axis = 0), bins = 100);\n",
    "ax2.set_xlim([0, 0.15])\n",
    "ax2.set_title('lambda')\n",
    "ax2.set_ylabel('Number of Customers')\n",
    "ax2.set_xlabel('lambda parameter')\n",
    "\n",
    "mu_la_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Comparisons Between Model Predictions and Training Set Observations \n",
    "\n",
    "Here we will perform a series of comparisons between the model predictions in the training period vs the observations over the same period of time. \n",
    "\n",
    "The fit above gives us all four model parameters $(r,\\alpha,s,\\beta)$. Furthermore, the MCMC chain also gives us the values of $\\lambda,\\mu$ for each customer. Given $(\\lambda,\\mu)$ it becomes relatively easy to derive the expected number of purchases made by each customer in the period $[T,T+t)$ : \n",
    "$$ E[Y(t)~|~\\lambda, \\mu] = \\frac{\\lambda}{\\mu}-\\frac{\\lambda}{\\mu}e^{-\\mu t } \\tag{4}$$\n",
    "\n",
    "In the expression above, Y(t) represents the number of (repeat) purchases between 0 and $t$. \n",
    "\n",
    "Parameters $\\lambda$ and $\\mu$ and latent parameters. They are not observed. However, the MCMC sampling technique allows us to extract the joint posterior distribution of $\\lambda$ and $\\mu$ at the customer level. This is very handy. Indeed, one simply has to evaluate the expression above for all the pairs of $\\lambda$ and $\\mu$ included in the MCMC trace. This is a fairly straightforward process. We show how one can do this in the cells below.  \n",
    "\n",
    "When making predictions about purchase count in the training period we also need to compute the probability of a customer being alive.  After all, a churned customer won't make any purchases.  To compute that probability we can use the following equation:\n",
    "\n",
    "$$P(\\tau > T|\\lambda, \\mu, x, t_x, T) = \\frac{1}{1+\\mu/(\\mu+\\lambda)[e^{(\\lambda + \\mu)(T-t_x)}-1]}$$\n",
    "\n",
    "Then, to compute the expected purchase count we use:\n",
    "\n",
    "$$ E[Y(t)~|~\\lambda, \\mu] \\cdot P(\\tau > T|\\lambda, \\mu, x, t_x, T)$$\n",
    "\n",
    "Note that for the training period we assume that this probability is equal to 1 (the customer has not churned in this period) and so in that case we can just use Equation 4, but for the validatio period we need the full equation shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Scatter Plot of the Purchase Counts : Observations vs Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training time in days\n",
    "dt_train = end_training_time.to_period('D') - transactions['date'].min().to_period('D') \n",
    "\n",
    "#From equation 4\n",
    "training_predictions = (la_dist/mu_dist-la_dist/mu_dist*np.exp(-mu_dist*dt_train)).mean(axis=0)\n",
    "\n",
    "rfm['model_train_frequency'] = training_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute RMSE in the training data\n",
    "rmse_train_count = (rfm['model_train_frequency'] - rfm['frequency']).apply(lambda x : x*x)\n",
    "rmse_train_count = np.sqrt(rmse_train_count.sum()/len(rfm))\n",
    "print('RMSE =', rmse_train_count)\n",
    "\n",
    "# Let's see the scatter plot predicted vs observed purchase counts in the training period. \n",
    "\n",
    "phf.plot_scatter(rfm, 'frequency', 'model_train_frequency', \n",
    "             xlabel='Observed Freq', ylabel = 'Fitted Freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "Not bad. The model is underestimating the frequency, but generally produces reasonable predictions.  Though this is expected given that this dataset was used to train the model. A more meaningful comparison would be done on a holdout period that the model has not seen yet. That's what we'll do below. \n",
    "\n",
    "I recommend adding to the diagnostics scatter plot above. A few suggestions include : \n",
    "* A chart showing the residuals per observed counts. This may indicate where the model performs poorly. \n",
    "* A cumulative distribution function (CDF) of the total number of purchases. This will be useful to determine whether or not the model can be used to forecast demand for this particular cohort of customers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Comparisons Between Predictions and the Holdout (validation) Set Observations\n",
    "\n",
    "As discussed below, we will now take a look at the holdout period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_alive_at_T(la, mu, t_x, T): \n",
    "    \"\"\"Computes the probability of being alive at T given lambda, mu, t_x, and T\"\"\"\n",
    "    return 1. / ( 1. + mu / (mu + la) * (np.exp((la + mu) * (T - t_x)) - 1.) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get length of holdout prediction period in terms of model period\n",
    "dt_holdout = transactions['date'].max().to_period('D') - end_training_time.to_period('D')\n",
    "\n",
    "T_values = np.tile(rfm['T'].values, [len(trace) - burn_in, 1])\n",
    "recency_values = np.tile(rfm['recency'].values, [len(trace) - burn_in, 1])\n",
    "\n",
    "# Holdout counts predictions - note we have a probability to be alive in this case: \n",
    "holdout_predictions = ((la_dist/mu_dist-la_dist/mu_dist*np.exp(-mu_dist*dt_holdout))*\\\n",
    "                        prob_alive_at_T(la_dist, mu_dist, recency_values, T_values)).mean(axis=0)\n",
    "\n",
    "rfm['frequency_predicted'] = holdout_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets look at the observed number of transactions during the same time period : \n",
    "# counts per customer per date : \n",
    "holdout_counts = holdout.groupby(['cust', 'date'], as_index=False).size().reset_index()\n",
    "\n",
    "# counts per customer\n",
    "# note that the holdout counts are also the frequency\n",
    "# because the 'first' purchase is not included in the holdout counts\n",
    "holdout_counts = holdout_counts.groupby(['cust']).size() \n",
    "\n",
    "# Let's merge with the rfm dataframe. \n",
    "rfm.drop(['obs_holdout_frequency'], inplace = True, errors = 'ignore', axis=1) # Remove this column if it already exists\n",
    "rfm = rfm.merge(pd.DataFrame(holdout_counts), how='left', left_on='cust', right_index = True)\n",
    "rfm.rename(columns={0:'obs_holdout_frequency'}, inplace=True)\n",
    "rfm.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now plot the data : \n",
    "# NOTE:  Here we compare frequency to 'count' \n",
    "rmse_holdout_count=(rfm['frequency_predicted']-rfm['obs_holdout_frequency']).apply(lambda x :x*x)\n",
    "rmse_holdout_count=np.sqrt(rmse_holdout_count.sum()/len(rfm))\n",
    "print('RMSE =',rmse_holdout_count)\n",
    "phf.plot_scatter(rfm, 'obs_holdout_frequency', 'frequency_predicted',\n",
    "            xlabel='Observed Freq', ylabel = 'Predicted Freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "Not surprisingly we're not doing as well on the holdout set than we did on the training set.\n",
    "\n",
    "There are several ways to improve the holdout results. Segmentation is one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trace, la_dist, mu_dist, T_values, recency_values\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Training a simple Gamma-Gamma model on the monetary value \n",
    "\n",
    "The next steps is to look at the monetary value model. This model follows closely the Gamma-Gamma model discussed in Fader et al. (2004). Note that the monetary value component is generally very difficult to model. Many factors can affect the price of items in ways that are not accounted for by the model. Long term changes in prices over several years (akin to inflation), discounts, promotions, etc. are difficult to capture in this and other simple monetary value models. That is something to keep in mind when doing comparisons over the holdout period and making predictions for future purchases. \n",
    "\n",
    "In the gamma model, the observed average order value in the training period is an imperfect metric of the latent mean transaction value $E(M)$ at the customer level. \n",
    "\n",
    "The main assumption behind the gamma model is that the average order value at the customer level is distributed according to a gamma distribution of shape $p$ and scale $\\nu$ \n",
    "$$ p(m_x~|~p, \\nu, x) = \\frac{(\\nu x)^{px}m_x^{px-1}e^{-\\nu x m_x}}{\\Gamma(px)} \\tag{5}$$\n",
    "\n",
    "where \n",
    "* $x$ is the total number of transactions (`rfm['frequency']+1`) \n",
    "* $m_x$ is the average order value\n",
    "* $p$ is the shape parameter of the gamma distribution. The model assumes that this parameter is the same for all customers. \n",
    "* $\\nu$ is the rate parameter of the gamma distribution. $\\nu$ varies across customers and has a prior that is also gamma distributed with parameters $(q,\\gamma)$\n",
    "\n",
    "The prior for $\\nu$ is given by:\n",
    "\n",
    "$$g(\\nu|q,\\gamma) = \\frac{\\gamma^q}{\\Gamma(q)}\\nu^{q-1}e^{-\\nu \\gamma} \\tag{6}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cust = len(rfm)\n",
    "\n",
    "with pm.Model() as gamma_gamma:\n",
    "    \n",
    "    #Define a bounded normal distribution\n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=np.array(0))\n",
    "    \n",
    "    p = BoundedNormal('p', mu = 10, sd = 100)    # prior distribution on p\n",
    "    q = BoundedNormal('q', mu = 10, sd = 100)    # hyperprior distribution on q \n",
    "    y = BoundedNormal('y', mu = 10, sd = 100)    # hyperprior distribution on y\n",
    "\n",
    "    # Implement the distribution for v as in Equation 6 and similar\n",
    "    # to the distributions on mu and lambda in the Pareto/NBD model\n",
    "    v = pm.Gamma('v', alpha = q, beta = y, shape = n_cust)\n",
    "        \n",
    "    # Here we multiply p * count.  the Deterministic() class allows us to make\n",
    "    # non-stochastic computations like these\n",
    "    px = pm.Deterministic('px', p * (rfm['frequency'].values + 1.0))\n",
    "    \n",
    "    # Compute v * x\n",
    "    nx = pm.Deterministic('nx', v * (rfm['frequency'].values + 1.0))\n",
    "\n",
    "    # Gamma likelihood as shown in Equation 5\n",
    "    mx = pm.Gamma('mx', alpha = px, beta = nx, observed = rfm['monetary_value'].values, shape = n_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "burn_in = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with gamma_gamma:\n",
    "#    trace_gg = pm.sample(draws = 20000, init = 'None', tune=burn_in, chain = 1, step = pm.NUTS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('gamma_gamma.pkl', 'wb') as f:\n",
    "#    dill.dump(trace_gg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#local_file = 'gamma_gamma.pkl'\n",
    "#target_file = 'expertise-seminars/LTV/gamma_gamma.pkl'\n",
    "#shf.push_file_to_s3(path = local_file, key = target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_file = '/home/jupyter/gamma_gamma.pkl'\n",
    "target_file = 'expertise-seminars/CLV/gamma_gamma.pkl'\n",
    "shf.pull_file_from_s3(target_file, local_file, use_creds = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/jupyter/gamma_gamma.pkl', 'rb') as f:\n",
    "    trace_gg = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Population Parameters:')\n",
    "print('----------------------')\n",
    "p = np.median(trace_gg[burn_in:].get_values(varname='p'))\n",
    "print('p:\\t\\t' + str(p))\n",
    "q = np.median(trace_gg[burn_in:].get_values(varname='q'))\n",
    "print('q:\\t\\t' + str(q))\n",
    "y = np.median(trace_gg[burn_in:].get_values(varname='y'))\n",
    "print('y:\\t\\t' + str(y))\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "ax1.hist(trace_gg[burn_in:].get_values(varname='p'), normed = True, bins = 100);\n",
    "ax1.set_title('p')\n",
    "ax1.vlines(p, 0, 1.5)\n",
    "\n",
    "ax2.hist(trace_gg[burn_in:].get_values(varname='q'), normed = True, bins = 100);\n",
    "ax2.set_title('q')\n",
    "ax2.vlines(q, 0, 4)\n",
    "\n",
    "ax3.hist(trace_gg[burn_in:].get_values(varname='y'), normed = True, bins = 100);\n",
    "ax3.set_title('y')\n",
    "ax3.vlines(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Customer-level Parameters:')\n",
    "print('--------------------------')\n",
    "# Note that we still need to take means\n",
    "# to get each parameter value, since each\n",
    "# user has a distribution for each user-specific parameter.\n",
    "\n",
    "p_dist = trace_gg[burn_in:].get_values(varname='p')\n",
    "v_dist = trace_gg[burn_in:].get_values(varname='v')\n",
    "\n",
    "gg_indiv_df = pd.DataFrame({'cust': rfm['cust'], \n",
    "                         'v:' : trace_gg[burn_in:].get_values(varname='v').mean(axis=0),\n",
    "                        'px:' : trace_gg[burn_in:].get_values(varname='px').mean(axis=0),\n",
    "                        'nx:' : trace_gg[burn_in:].get_values(varname='nx').mean(axis=0)})\n",
    "\n",
    "\n",
    "# Customer-level parameters\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.hist(trace_gg[burn_in:].get_values(varname='v').mean(axis = 0), bins = 100);\n",
    "ax1.set_title('v')\n",
    "ax1.set_ylabel('Number of Customers')\n",
    "ax1.set_xlabel('v parameter')\n",
    "\n",
    "ax2.hist(trace_gg[burn_in:].get_values(varname='px').mean(axis = 0), bins = 100);\n",
    "ax2.set_title('px')\n",
    "ax2.set_ylabel('Number of Customers')\n",
    "ax2.set_xlabel('px parameter')\n",
    "\n",
    "ax3.hist(trace_gg[burn_in:].get_values(varname='nx').mean(axis = 0), bins = 100);\n",
    "ax3.set_title('nx')\n",
    "ax3.set_ylabel('Number of Customers')\n",
    "ax3.set_xlabel('nx parameter')\n",
    "\n",
    "gg_indiv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Computing E(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, this model will give us the expected average transaction value for a customer with an average spend of $m_x$ dolalrs across $x$ transactions in the training period. In this case, the expectation value of the gamma distribution, which is the mean purchase value is:\n",
    "\n",
    "$$E(M) = p/\\nu \\tag{7}$$\n",
    "\n",
    "The MCMC sampling technique gives us the posterior distribution of $\\nu$. To get an estimate of $E(M)$ at the customer level, one simply has to average the value of $p/\\nu$ over the MCMC trace values of $\\nu$ and $p$, at the customer level. We go over the steps in the cells below. \n",
    "\n",
    "\n",
    "$E(M)$ will then be multiplied by $E[Y(t)~|~\\lambda, \\mu, \\rm{alive~at~T}]$ to give us the CLV of each customer in the holdout period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_M = (p_dist / v_dist.T).mean(axis=1)\n",
    "\n",
    "rfm.drop(['monetary_value_predicted'], inplace = True, errors = 'ignore', axis=1) # Remove this column if it already exists\n",
    "rfm = rfm.merge(pd.DataFrame(E_M), how='left', left_index=True, right_index=True)\n",
    "rfm.rename(columns={0:'monetary_value_predicted'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Comparisons between E(M) and observed mean in training period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfm[['monetary_value', 'monetary_value_predicted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's explore the results : \n",
    "\n",
    "phf.plot_scatter(rfm,'monetary_value','monetary_value_predicted', \n",
    "             xlabel='Average Order Value in Training Period ($)', \n",
    "             ylabel='E(M) ($)', \n",
    "             xlim=[0,50], ylim=[0,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "In the figure above, what can be perceived as different \"lines\" correspond to different values of \"x\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Comparisons between E(M) and observed mean in holdout/validation period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's compute the observed mean transaction value per customer in the holdout period : \n",
    "\n",
    "holdout_value = holdout.groupby(['cust', 'date'], as_index=False)['sales'].sum().reset_index()\n",
    "holdout_value = holdout_value[['cust', 'sales']].groupby(['cust'], as_index = False)['sales'].mean()\n",
    "holdout_value=pd.DataFrame(holdout_value)\n",
    "holdout_value.rename(columns={'sales':'obs_holdout_monetary_value'}, inplace=True)\n",
    "\n",
    "# merge with rfm dataframe : \n",
    "rfm.drop(['obs_holdout_monetary_value'], inplace = True, errors = 'ignore', axis=1) # Remove this column if it already exists\n",
    "rfm = rfm.merge(holdout_value, how='left', left_on='cust', right_on='cust')\n",
    "rfm.fillna(0,inplace=True)\n",
    "\n",
    "# Note that we don't need to compute an E(M) for the holdout period\n",
    "# Predicted E(M) is the same for all time periods because it's a customer's\n",
    "# expected average order values for all orders, ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfm[['obs_holdout_monetary_value', 'monetary_value_predicted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phf.plot_scatter(rfm,'obs_holdout_monetary_value','monetary_value_predicted', \n",
    "             xlabel='Average Order Value in holdout Period ($)', \n",
    "             ylabel='E(M) ($)', \n",
    "             xlim=[-1,80], ylim=[-1,80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "This chart above highlights how difficult it is to accurately model the monetary value. Most of the data points are found along the observed value of 0. This is because most customers did not make a purchase in the holdout period. For the ones who did, the scatter is very large. The model tends to overestimate the monetary value in the holdout period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# Finally, computing the CLV in the holdout period and comparing with with model predictions \n",
    "\n",
    "We are at the end of this exercise. The last step is to compute the customer-level CLV predictions for the holdout period. I invite you to do the same comparison for the training period. \n",
    "\n",
    "CLV is obtained by \n",
    "$$CLV(T,T+t) = E(M)~\\times~ E[Y(t)~|~\\lambda, \\mu, \\rm{alive~at~T}] \\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute both modeled and observed CLV in the holdout period : \n",
    "\n",
    "# The model-predicted CLV\n",
    "rfm['model_holdout_clv'] = rfm['frequency_predicted'] * rfm['monetary_value_predicted']\n",
    "\n",
    "# Compute the observed CLV\n",
    "rfm['obs_holdout_clv'] = rfm['obs_holdout_frequency'] * rfm['obs_holdout_monetary_value']\n",
    "\n",
    "\n",
    "rmse_holdout_clv = (rfm['model_holdout_clv'] - rfm['obs_holdout_clv'])* \\\n",
    "                   (rfm['model_holdout_clv'] - rfm['obs_holdout_clv'])\n",
    "rmse_holdout_clv = np.sqrt(rmse_holdout_clv.sum()/len(rfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the final results : \n",
    "print('RMSE =', rmse_holdout_clv)\n",
    "phf.plot_scatter(rfm, 'obs_holdout_clv', 'model_holdout_clv',\n",
    "             xlabel='Observed Customer Value in the Holdout Period',\n",
    "             ylabel='Modeled Customer Value in the Holdout Period', \n",
    "             xlim=[-1,300.0],ylim=[-1,300.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "source": [
    "# DS_CLV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = 'expertise-seminars/CLV/Modules/'\n",
    "shf.s3_fetch_module(s3_path = model_dir, file_name = 'ds_clv.tar.gz', use_creds = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install lifetimes\n",
    "!pip install pystan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ds_clv.rfm import RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create the RFM dataframe\n",
    "# We don't need to write a function this time\n",
    "# The ds_clv package can do this for us\n",
    "\n",
    "rfm2 = RFM.from_transaction_log(transactions, customer_id_col='cust',\n",
    "    datetime_col='date', calibration_period_end=end_training_time,\n",
    "    monetary_value_col='sales', freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfm2.rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a pareto-ndb model using a gamma-gamma model for monetary value\n",
    "# We'll use MLE method to fit it instead of MCMC, which is much faster\n",
    "\n",
    "# Import the ParetoNBD class, using MLE method to fit\n",
    "from ds_clv.transactional import ParetoNBDMLE\n",
    "\n",
    "# Initialize the model\n",
    "pnbd = ParetoNBDMLE()\n",
    "\n",
    "# Fit the Pareto/NBD model, as well as the \n",
    "# gamma-gamma model for monetary value\n",
    "# This single line replaces both of the models we previously created\n",
    "pnbd.fit(rfm2, monetary_value_model_type='gg_mle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the parameter estimates\n",
    "\n",
    "print('Pareto/NBD params:')\n",
    "print('------------------')\n",
    "r = pnbd.params[0]\n",
    "print('r:\\t\\t' + str(r))\n",
    "alpha = pnbd.params[1]\n",
    "print('alpha:\\t\\t' + str(alpha))\n",
    "s = pnbd.params[2]\n",
    "print('s:\\t\\t' + str(s))\n",
    "beta = pnbd.params[3]\n",
    "print('beta:\\t\\t' + str(beta))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('GG Params:')\n",
    "print('----------')\n",
    "p = pnbd.monetary_value_model.params[0]\n",
    "print('p:\\t\\t' + str(p))\n",
    "q = pnbd.monetary_value_model.params[1]\n",
    "print('q:\\t\\t' + str(q))\n",
    "y = pnbd.monetary_value_model.params[2]\n",
    "print('y:\\t\\t' + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the prediction DataFrame\n",
    "# Again, just a single line replaces\n",
    "# many lines of code we used to predict\n",
    "\n",
    "# Note that the input is just an integer representing\n",
    "# the number of days.  The model will return the predicted\n",
    "# frequency and monetary value for each of the customers \n",
    "# on which it were trained for that number of days\n",
    "pred_df = pnbd.predict(dt_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make a new data frame and merge the predictions with the RFM dataframe\n",
    "res2_df = pred_df.copy()\n",
    "res2_df.reset_index(level=0, inplace=True)\n",
    "res2_df = res2_df.merge(rfm2.rfm, left_on = 'cust', right_index=True)\n",
    "\n",
    "\n",
    "# Let's merge the holdout data in as well \n",
    "res2_df.drop(['obs_holdout_frequency'], inplace = True, errors = 'ignore', axis=1) # Remove this column if it already exists\n",
    "res2_df = res2_df.merge(pd.DataFrame(holdout_counts), how='left', left_on = 'cust', right_index = True)\n",
    "res2_df.rename(columns={0:'obs_holdout_frequency'}, inplace=True)\n",
    "res2_df.fillna(0, inplace=True)\n",
    "\n",
    "res2_df = res2_df.merge(holdout_value, how='left', left_on='cust', right_on = 'cust')\n",
    "res2_df.fillna(0,inplace=True)\n",
    "\n",
    "# Let's see how well the model's predicted frequency matches that observed in the holdout\n",
    "\n",
    "rmse_holdout_count=(res2_df['frequency_predicted']-res2_df['obs_holdout_frequency']).apply(lambda x :x*x)\n",
    "rmse_holdout_count=np.sqrt(rmse_holdout_count.sum()/len(res2_df))\n",
    "print('RMSE =',rmse_holdout_count)\n",
    "phf.plot_scatter(res2_df, 'obs_holdout_frequency', 'frequency_predicted', \n",
    "             xlabel='Observed Freq', ylabel='Predicted Freq')\n",
    "\n",
    "phf.plot_scatter(res2_df,'obs_holdout_monetary_value','monetary_value_predicted', \n",
    "             xlabel='Average Order Value in holdout Period ($)', \n",
    "             ylabel='E(M) ($)', \n",
    "             xlim=[-1,80], ylim=[-1,80])\n",
    "\n",
    "# Finally, let's compute the CLV\n",
    "\n",
    "# Compute the model-predicted and observed CLV\n",
    "# Refer to previous MCMC section \n",
    "res2_df['model_holdout_clv'] = res2_df['frequency_predicted'] * res2_df['monetary_value_predicted']\n",
    "res2_df['obs_holdout_clv'] = res2_df['obs_holdout_frequency'] * res2_df['obs_holdout_monetary_value']\n",
    "\n",
    "\n",
    "rmse_holdout_clv = (res2_df['model_holdout_clv'] - res2_df['obs_holdout_clv'])* \\\n",
    "                   (res2_df['model_holdout_clv'] - res2_df['obs_holdout_clv'])\n",
    "rmse_holdout_clv = np.sqrt(rmse_holdout_clv.sum()/len(res2_df))\n",
    "\n",
    "# plot the final results : \n",
    "print('RMSE =', rmse_holdout_clv)\n",
    "phf.plot_scatter(res2_df, 'obs_holdout_clv', 'model_holdout_clv',\n",
    "             xlabel='Observed Value in the Holdout Period',\n",
    "             ylabel='Modeled Value in the Holdout Period', \n",
    "             xlim=[-1,300.0],ylim=[-1,300.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "# In conclusion \n",
    "\n",
    "The Pareto/NBD model is a good introduction to CLV modeling. The gamma-gamma model for monetary value is easy and simple to train. \n",
    "\n",
    "There are however several limitations to these approaches. More sophisticated models could include user segmentation. Bayes hierarchical models may also be able to better discriminate groups of customers who exhibit different behaviors and model their $\\lambda$ and $\\mu$ accordingly. \n",
    "\n",
    "The team of data scientists at DataScience can help you designing and improve these models. For our DataScience Cloud Platform customers, we provide a CLV playbook along with a library of CLV models, diagnostics and support. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort-based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Time series of the number of transactions (daily) \n",
    "plt.rcParams['figure.figsize'] = (20, 5)\n",
    "ts_transactions = transactions.groupby(['date']).size()\n",
    "plt.ylabel('Number of Transactions')\n",
    "ts_transactions.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "The chart above looks at the number of transactions per day. As you probably noticed, there is a sharp break in the counts at the end of `1997-03`. That is because the dataset only includes the customers who made their first purchase in the first quarter of 1997. That is, in this data set we are acquiring new customers from 1997-01 until the end of 1997-Q1, and the new customer transactions are included in this period.  But we stop including new customers post 1997-Q1 in the data, so transactions from new customers are no longer included after this date, leading to the discontinuous drop in transaction count.  If the dataset would have also included the customers who made their first purchase post 1997-Q1, the count per day would have likely continued to increase. \n",
    "\n",
    "This is a standard practice when modeling CLV. The cohorts of customers used to train the models are generally based on their **time of first purchase**. That way, one can study the evolution of the population parameters over time and pinpoint possible problems in the long run. That is, changes in the business, the product, marketing, etc may change how customer transactions evolve from the time of first purchase onward, and by splitting customers into cohorts based on time of first purchase we can monitor these effects.\n",
    "\n",
    "The drawback of using such cohort-based approach is that you have fewer datapoints for each cohort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Computing Interpurchase Time (IPT)\n",
    "\n",
    " The IPT is useful to look at to determine whether the NBD model should be used\n",
    "\n",
    " Let's take a look at the typical IPT and IPT distribution \n",
    "\n",
    " To compute interpurchase time we need to compute the difference \n",
    " between each purchase and the subsequent purchase that follows\n",
    "\n",
    " transaction_time_n - transaction_time_(n-1)\n",
    " \n",
    " We can simply sort by date, then group by the customer ID, and finally compute the difference between each date\n",
    " \n",
    " We'll finally drop the NaT (Not a Time) values, which are generated for the final purchase of each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the transaction's data frame\n",
    "transactions_tmp = transactions.copy()\n",
    "\n",
    "# Compute IPT in days\n",
    "transactions_tmp['IPT(days)'] = transactions_tmp.sort_values('date', ascending = True).groupby(['cust'], as_index=False)[['date']].diff()\n",
    "\n",
    "# Drop rows that don't have an IPT (e.g. if it's the last purchase)\n",
    "transactions_tmp = transactions_tmp[~transactions_tmp['IPT(days)'].isnull()]\n",
    "\n",
    "# Convert IPT from type timedelta to type integer\n",
    "transactions_tmp['IPT(days)'] = transactions_tmp['IPT(days)'].apply(lambda l: l.days)\n",
    "\n",
    "transactions_tmp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's print the mean IPT. Our training period of 365 days is long enough. \n",
    "\n",
    "print(transactions_tmp['IPT(days)'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Distribution of IPT\n",
    "\n",
    "The IPT distribution can tell us whether the Pareto/NBD model assumptions are being met.  Specifically, we expect that the log density of the IPT distribution should be linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribution of IPT : \n",
    "\n",
    "transactions_tmp['IPT(days)'].hist(bins=40)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('IPT (days)') \n",
    "plt.ylabel('Number of Purchases') \n",
    "\n",
    "# 275 (365-90) days to avoid right censorship issues. \n",
    "plt.xlim([0,270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute log density of the interpurchase time\n",
    "# \n",
    "vals, bins = np.histogram(transactions_tmp['IPT(days)'].values, bins=40)\n",
    "log_density = np.log(vals / vals.sum())\n",
    "log_density[np.isneginf(log_density)] = 0\n",
    "bins=bins[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit a line ot the log density\n",
    "from scipy.stats import linregress\n",
    "density_fit = linregress(bins, log_density)\n",
    "ydens_fit = (bins * density_fit.slope) + density_fit.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Fit R^2: ' + str(density_fit.rvalue**2))\n",
    "plt.plot(bins, log_density)\n",
    "plt.plot(bins, ydens_fit)\n",
    "plt.xlabel('IPT (Days)')\n",
    "plt.ylabel('Log Density')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "## Distribution of Number of Purchases Per Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the number of purchases per customer : \n",
    "\n",
    "n_purchases = transactions.groupby(['cust']).size()\n",
    "print(n_purchases.min(axis=0), n_purchases.max(axis=0))\n",
    "n_purchases.hist(bins=(n_purchases.max(axis=0) - n_purchases.min(axis=0)) + 1)\n",
    "plt.xlabel('Number of Purchases') \n",
    "plt.ylabel('Number of Customers') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "As we see in the figure above, more than 50% (1200/2357) of the customers made only a single purchase in the 1.5 year period covered by the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "_datascience": {
   "notebookId": 823
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
